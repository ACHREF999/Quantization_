{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB1X1Jcg4pnD"
      },
      "outputs": [],
      "source": [
        "# Depp nns usually have many params (millions , billions ..)\n",
        "# which can require too many space if we are simply storing each param\n",
        "# in 32bits , (7b -> 28GB)!! and this makes even the inference of the\n",
        "# model on simple not so powerfull devices not feasible\n",
        "# computers are (like humns) are slow in computing floating-point ops\n",
        "# compared to integer ops , 2*2 vs 2.123*1.258\n",
        "\n",
        "# Quantization aims to reduce the total amount of bits required to\n",
        "# represent each param , usually by converting floating-point numbers to\n",
        "# integeres , but not as simply rounding up or down the numbers ,\n",
        "# this will result in more compressed representation of the model params\n",
        "# which helps with using /training the model\n",
        "\n",
        "# Quantization can also speed up computation because using simpler data\n",
        "# types is faster (earlier example )\n",
        "\n",
        "# in short :\n",
        "\"\"\"\n",
        "- less storage space required\n",
        "- less computation\n",
        "- less inference time\n",
        "- less energy consumption\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# cpus and gpus use fixed number of bits to represent a piece of (primitive)\n",
        "# data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2**999"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TonmHqAK7KdN",
        "outputId": "f4d2b8e5-08e6-4f3a-8332-06fb6868192f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5357543035931336604742125245300009052807024058527668037218751941851755255624680612465991894078479290637973364587765734125935726428461570217992288787349287401967283887412115492710537302531185570938977091076523237491790970633699383779582771973038531457285598238843271083830214915826312193418602834034688"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You suprised that worked ?\n",
        "# well python can represent big numbers by leveraging BigNum arithmetic\n",
        "# BigNum is for working with large numbers that dont fit in 32/64 bits\n",
        "# well python uses array of integers for large enough numbers\n",
        "# a the number grows it keeps adding chunks\n",
        "# so for BigNums it doesnt use cpu instruction like add , sub ...\n",
        "# it implements this manual algorithms\n",
        "# eg. in addition u add together small chunks nd carry over the overflow\n",
        "# to the next chunk\n",
        "# for mult it uses smarter algos :\n",
        "# Karatsuba algorithm\n",
        "# FFT-based Multiplication\n",
        "# The Python interpreter is what calculating those NOT CPU\n",
        "\n",
        "# remember the IEEE-754 ? lol first year trauma\n",
        "# anyway it defines floating point numbers representation in 32bits as follows :\n",
        "# first bit at left is for the sign\n",
        "# the next 8 bits are the exponent\n",
        "# the last are the fraction powers (first bit is 2^-1)\n",
        "\n",
        "# val = (-1)**sign * 2^(E-127) * (1+ SUM[i=1..23, B[23-i]*2^(-i)])\n",
        "# This is more like the scientific representation we studied in highschool or wtver\n",
        "# to represent lets say (+/-)xxx.xx u need to write it in the form :\n",
        "# (-1)^S * x.xxxx * 2^E  ; the bits after the dot are the mantissa bits (23 bs) ,\n",
        "# S is the sign bit\n",
        "\n",
        "\n",
        "# The exponent in the IEEE 754 format is stored as an unsigned integer, but we often need\n",
        "# both positive and negative exponents to represent small and large numbers.\n",
        "# So, instead of using a signed exponent,\n",
        "# we use an unsigned exponent and add a bias (a fixed number) to shift the range.\n",
        "\n",
        "\n",
        "# This means:\n",
        "# Stored exponent e = actual_exponent + 127\n",
        "# So, to get the real exponent:actual¬†exponent=ùëí‚àí127\n",
        "\n",
        "# floating-point numbers scale much more because\n",
        "# they use the exponent to jump to big or small ranges.\n",
        "\n",
        "\n",
        "# Simple CPU ADD and MUL Instructions Don't Work on Floating-Point Numbers Directly.\n",
        "# Instead, they require specialized hardware and instructions.\n",
        "# Modern CPUs have a Floating Point Unit (FPU) ‚Äî a dedicated part\n",
        "# of the processor that knows\n",
        "# CPUs (like x86) and GPUs use dedicated instructions for floating-point math.\n",
        "\n",
        "\n",
        "# GPUs also support 16-bit floating point number with less precision"
      ],
      "metadata": {
        "id": "Ey_AfYrC7NRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USUALLY in nn , weights and biases are represented using floating-point numbers\n",
        "# Quantization tries to use integer numbers to represent these two matrices while\n",
        "# maintaining the accuracy of the model\n",
        "# The Goal is to Quantize Input , Weight , Bias into the integer space\n",
        "# so we perform all operations using integer arithmetic\n",
        "# then we take the output , Dequantize , and send it to the next layer\n",
        "# we need the next layer not even realize we quantized the previous\n",
        "# so we should not CHANGE the model's output using quantization\n",
        "# we need a mapping between floating points and ints without losing acc , meaning ...\n",
        "\n",
        "# WELL , as u guessed it , we lose some information ; (info theo bb)\n",
        "# we are trying to `compress` a huge CONITNUOUS domain , into a\n",
        "# small DISCRETE domain [-127,127] , (we usually sacrifice the -128 to obtain a\n",
        "# symetric domain/range)\n",
        "# what we a re trying to do is keep the same distribution as the floating point\n",
        "# numbers with respect to their domain (the actuall domain and not the possible domain)\n",
        "# [MIN(float_weights),MAX(float_weights)] ,\n",
        "# and we keep an ANCHOR that tells us how we scaled and how to scale back called the\n",
        "# `zero_point` (idk if this name only holds for asymemtric quantization)\n",
        "# there are two types of quantization\n",
        "# asymetric : the zero of one representation not necessarly the zero of the other\n",
        "# symetric : the zero is same for both when [min == max]\n",
        "\n",
        "# In asymmetric quantization\n",
        "# Xq = clamp(floor(xf/s),0,2^n -1) ; s = (alpha-beta)/(2^n -1)\n",
        "\n",
        "# alpha the biggest float in the tensor , Beta the smallest\n",
        "# we center using the z parameter\n",
        "# z = round(-1 * (Beta/s))\n",
        "# BIGGEST NUMBER MAPPED TO BIGGEST NUMBER\n",
        "# Smallest number mapped to 0\n",
        "# 0 mapped to the center point z\n",
        "\n",
        "# dequantization  : Xf = s(Xq - z)\n",
        "# we notice some information loss\n",
        "\n",
        "# In symmetric quantization\n",
        "# Xq = clamp(round(Xf/s),-(2^(n-1) -1) , (2^(n-1) -1) )\n",
        "# alpha the biggest value in absolute form\n",
        "# s = abs(alpha)/(2^(n-1) -1)\n",
        "\n"
      ],
      "metadata": {
        "id": "5y2hlnchOJZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# suppress scientific notation\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate randomly\n",
        "X = np.random.uniform(low=-50,high=150 , size=20)\n",
        "\n",
        "\n",
        "# For debugging purposes , lets make sure the important values are at the beggining\n",
        "X[0] = X.max() + 1\n",
        "X[1] = X.min() - 1\n",
        "X[2] = 0\n",
        "\n",
        "\n",
        "# Only keep two decimal places\n",
        "X = np.round(X,2)\n",
        "\n",
        "\n",
        "print(X)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itK2QO7ssNJK",
        "outputId": "9ca2d9d2-7c93-4d6b-96c5-c0c89dc8837e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[140.92 -49.87   0.    55.88 -48.87 132.91 -27.78 -22.58 105.08  17.84\n",
            "  60.82 115.05 139.92  78.98  38.04  46.68  69.47  54.5   -5.47 -22.71]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple str8 forward\n",
        "def clamp(params,lower_bound,upper_bound):\n",
        "    params[params < lower_bound] = lower_bound\n",
        "    params[params > upper_bound] = upper_bound\n",
        "    return params\n",
        "\n",
        "# Xq = clamp(floor(xf/s),0,2^n -1) ; s = (alpha-beta)/(2^n -1)\n",
        "# z = round(-1 * (Beta/s))\n",
        "\n",
        "def asymmetric_quantization(X,num_bits):\n",
        "    s = (X.max() - X.min() )/ (2**num_bits -1)\n",
        "    z = np.round(-1*(X.min()/s))\n",
        "    Xq = clamp(np.round(X/s + z),0 , 2**num_bits -1).astype(np.int32)\n",
        "\n",
        "    return Xq, s, z\n",
        "\n",
        "# dequantization  : Xf = s(Xq - z)\n",
        "def asymmetric_dequantization(Xq,scale,z):\n",
        "    Xf = scale * (Xq - z)\n",
        "\n",
        "    return Xf\n",
        "\n",
        "# Xq = clamp(round(Xf/s),-(2^(n-1) -1) , (2^(n-1) -1) )\n",
        "# alpha the biggest value in absolute form\n",
        "# s = abs(alpha)/(2^(n-1) -1)\n",
        "\n",
        "def symmetric_quantization(X,num_bits):\n",
        "    alpha = np.max(np.abs(X))\n",
        "    s = np.abs(alpha) / (2**(num_bits-1)-1)\n",
        "    lower_bound , upper_bound = -(2**(num_bits-1)-1) , (2**(num_bits-1)-1)\n",
        "    Xq = clamp(np.round(X/s),lower_bound,upper_bound)\n",
        "\n",
        "\n",
        "    return Xq , s\n",
        "\n",
        "# Xf = Xq * scale\n",
        "def symmetric_dequantiztion(Xq,s):\n",
        "    return Xq * s\n",
        "\n",
        "\n",
        "\n",
        "def quantization_error(X,Xq):\n",
        "\n",
        "    return np.mean((X-Xq)**2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "asym_q , asym_s,asym_z = asymmetric_quantization(X,8)\n",
        "sym_q , sym_s = symmetric_quantization(X,8)\n",
        "\n",
        "\n",
        "\n",
        "print(f'original : \\n{np.round(X,2)}\\n\\n')\n",
        "print(f'Asymmetric scale : {asym_s} , zero : {asym_z} \\n{asym_q}\\n\\n')\n",
        "print(f'Symmetric scale : {sym_s}\\n{sym_q}')\n",
        "\n",
        "# Notice in Sym , alot of range is unused\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHhwEVrctNwz",
        "outputId": "88d7aa95-fd04-4f42-fc9d-5c7c3994ee08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original : \n",
            "[140.92 -49.87   0.    55.88 -48.87 132.91 -27.78 -22.58 105.08  17.84\n",
            "  60.82 115.05 139.92  78.98  38.04  46.68  69.47  54.5   -5.47 -22.71]\n",
            "\n",
            "\n",
            "Asymmetric scale : 0.7481960784313725 , zero : 67.0 \n",
            "[255   0  67 142   2 245  30  37 207  91 148 221 254 173 118 129 160 140\n",
            "  60  37]\n",
            "\n",
            "\n",
            "Symmetric scale : 1.1096062992125983\n",
            "[127. -45.   0.  50. -44. 120. -25. -20.  95.  16.  55. 104. 126.  71.\n",
            "  34.  42.  63.  49.  -5. -20.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asym_deq_q = asymmetric_dequantization(asym_q,asym_s,asym_z)\n",
        "sym_deq_q = symmetric_dequantiztion(sym_q,sym_s)\n",
        "\n",
        "\n",
        "print(f'original : \\n{np.round(X,2)}\\n\\n')\n",
        "print(f'Asymmetric :{asym_deq_q}\\n\\n')\n",
        "print(f'Symmetric  : {sym_deq_q}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klni3V9W2D8I",
        "outputId": "460cca95-8158-44e2-d568-9f416a84e830"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original : \n",
            "[140.92 -49.87   0.    55.88 -48.87 132.91 -27.78 -22.58 105.08  17.84\n",
            "  60.82 115.05 139.92  78.98  38.04  46.68  69.47  54.5   -5.47 -22.71]\n",
            "\n",
            "\n",
            "Asymmetric :[140.66086275 -50.12913725   0.          56.11470588 -48.6327451\n",
            " 133.17890196 -27.6832549  -22.44588235 104.74745098  17.95670588\n",
            "  60.60388235 115.22219608 139.91266667  79.30878431  38.158\n",
            "  46.38815686  69.58223529  54.61831373  -5.23737255 -22.44588235]\n",
            "\n",
            "\n",
            "Symmetric  : [140.92       -49.93228346   0.          55.48031496 -48.82267717\n",
            " 133.15275591 -27.74015748 -22.19212598 105.41259843  17.75370079\n",
            "  61.02834646 115.39905512 139.8103937   78.78204724  37.72661417\n",
            "  46.60346457  69.90519685  54.37070866  -5.5480315  -22.19212598]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(X,asym_deq_q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKFU6AUl2pze",
        "outputId": "3350e5d2-fea3-4fe6-865e-143ae6b27a95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.04518108581314796)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(X,sym_deq_q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsAgEXf42ywb",
        "outputId": "a8806012-9459-4682-ef1f-e2f81f71c27b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.06479050251100521)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In a trained nn , W , B are quantized , the input is quantized on the fly\n",
        "# but how do we dequantize the Y ?\n",
        "# well we run inference on the model on few inputs and observe typical outputs\n",
        "# to calculate scale and zero , this is called `callibration`\n",
        "# then we can dequantize the output of operations on the other quantized values\n",
        "# using the params we just learned\n",
        "\n",
        "# GPUs actually speed up linear layer using Multiply-Accumulate (MAC)\n",
        "# this op is performed in parallel for every row and column using many MAC blocks\n",
        "# GEMM library"
      ],
      "metadata": {
        "id": "VhQ35gr63Dej"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategies to choosing [Alpha , Beta] (the params of the scale)\n",
        "# - MinMax as we did up . This is sensitive to outliers, the error big for all but outlier\n",
        "# - Percentile , we dont rely on the outlier , the error big for the outlier\n",
        "# - MSE : GRID search to find those two that minimize MSE\n",
        "# - Cross-Entropy : create a proba distribution , choose alpha and beta such that\n",
        "# softmax(X) and softmax(Xq) is the same/very close\n",
        "\n",
        "# in CNN ,for each kernel is better to have alpha and beta , to not throw range\n",
        "\n"
      ],
      "metadata": {
        "id": "HO5bf1WN34cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "params = np.random.uniform(low=-50,high=150,size=10000)\n",
        "\n",
        "# outlier    VVVV\n",
        "params[-1] = 1000\n",
        "\n",
        "params = np.round(params,2)\n",
        "\n",
        "def asymmetric_quantization_percentile(X,num_bits,percentile=99.99):\n",
        "    alpha = np.percentile(X,percentile)\n",
        "    beta = np.percentile(X,100-percentile)\n",
        "    s = (alpha-beta) / (2**num_bits -10)\n",
        "    z = -1*np.round(beta/s)\n",
        "    lower_bound , upper_bound = 0 , 2**num_bits -1\n",
        "    quantized = clamp(np.round(X/s +z),lower_bound,upper_bound).astype(np.int32)\n",
        "    return quantized , s , z\n",
        "\n",
        "asym_q , asym_s,asym_z = asymmetric_quantization(params,8)\n",
        "asymp_q , asymp_s,asymp_z = asymmetric_quantization_percentile(params,8)\n",
        "\n",
        "print(f'original : \\n{np.round(params,2)}\\n\\n')\n",
        "print(f'Asymmetric scale : {asym_s} , zero : {asym_z} \\n{asym_q}\\n\\n')\n",
        "print(f'Asymmetric scale : {asymp_s} , zero : {asymp_z} \\n{asymp_q}\\n\\n')\n",
        "\n",
        "\n",
        "asym_deq_q = asymmetric_dequantization(asym_q,asym_s,asym_z)\n",
        "asymp_deq_q = asymmetric_dequantization(asymp_q,asymp_s,asymp_z)\n",
        "\n",
        "\n",
        "\n",
        "print(f'original : \\n{np.round(X,2)}\\n\\n')\n",
        "print(f'Asymmetric :{asym_deq_q}\\n\\n')\n",
        "print(f'Asymmetric P  : {asymp_deq_q}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFOCDG0f6IEa",
        "outputId": "ff3e0aea-1052-4dc0-8577-734707dbf5fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original : \n",
            "[ 131.24   61.94   -0.56 ...    3.4    -9.99 1000.  ]\n",
            "\n",
            "\n",
            "Asymmetric scale : 4.117529411764706 , zero : 12.0 \n",
            "[ 44  27  12 ...  13  10 255]\n",
            "\n",
            "\n",
            "Asymmetric scale : 0.8131097682902392 , zero : 61.0 \n",
            "[222 137  60 ...  65  49 255]\n",
            "\n",
            "\n",
            "original : \n",
            "[140.92 -49.87   0.    55.88 -48.87 132.91 -27.78 -22.58 105.08  17.84\n",
            "  60.82 115.05 139.92  78.98  38.04  46.68  69.47  54.5   -5.47 -22.71]\n",
            "\n",
            "\n",
            "Asymmetric :[ 131.76094118   61.76294118    0.         ...    4.11752941   -8.23505882\n",
            " 1000.55964706]\n",
            "\n",
            "\n",
            "Asymmetric P  : [130.91067269  61.79634239  -0.81310977 ...   3.25243907  -9.75731722\n",
            " 157.74329505]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(params[:-1],asym_deq_q[:-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OeHTapA77pE",
        "outputId": "5937adfb-ac49-43c1-fb65-7f16d8c67d20"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.4216433527615735)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(params[:],asymp_deq_q[:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khcS8u708lQ-",
        "outputId": "15e6c3c1-cb34-457c-8385-c7994d498c05"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(70.99464757401762)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(params[:-1],asymp_deq_q[:-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJwxBFc67-N3",
        "outputId": "57165ccd-13fa-4f00-dfe6-41092f625f1b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.055017372146425755)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}