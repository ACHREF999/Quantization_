{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GB1X1Jcg4pnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cadd4487-05e2-44a6-8959-16e8b8ef4f32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n- less storage space required\\n- less computation\\n- less inference time\\n- less energy consumption\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Depp nns usually have many params (millions , billions ..)\n",
        "# which can require too many space if we are simply storing each param\n",
        "# in 32bits , (7b -> 28GB)!! and this makes even the inference of the\n",
        "# model on simple not so powerfull devices not feasible\n",
        "# computers are (like humns) are slow in computing floating-point ops\n",
        "# compared to integer ops , 2*2 vs 2.123*1.258\n",
        "\n",
        "# Quantization aims to reduce the total amount of bits required to\n",
        "# represent each param , usually by converting floating-point numbers to\n",
        "# integeres , but not as simply rounding up or down the numbers ,\n",
        "# this will result in more compressed representation of the model params\n",
        "# which helps with using /training the model\n",
        "\n",
        "# Quantization can also speed up computation because using simpler data\n",
        "# types is faster (earlier example )\n",
        "\n",
        "# in short :\n",
        "\"\"\"\n",
        "- less storage space required\n",
        "- less computation\n",
        "- less inference time\n",
        "- less energy consumption\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# cpus and gpus use fixed number of bits to represent a piece of (primitive)\n",
        "# data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2**999"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TonmHqAK7KdN",
        "outputId": "cc77d335-8607-47f7-fd87-821ab3f94330"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5357543035931336604742125245300009052807024058527668037218751941851755255624680612465991894078479290637973364587765734125935726428461570217992288787349287401967283887412115492710537302531185570938977091076523237491790970633699383779582771973038531457285598238843271083830214915826312193418602834034688"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You suprised that worked ?\n",
        "# well python can represent big numbers by leveraging BigNum arithmetic\n",
        "# BigNum is for working with large numbers that dont fit in 32/64 bits\n",
        "# well python uses array of integers for large enough numbers\n",
        "# a the number grows it keeps adding chunks\n",
        "# so for BigNums it doesnt use cpu instruction like add , sub ...\n",
        "# it implements this manual algorithms\n",
        "# eg. in addition u add together small chunks nd carry over the overflow\n",
        "# to the next chunk\n",
        "# for mult it uses smarter algos :\n",
        "# Karatsuba algorithm\n",
        "# FFT-based Multiplication\n",
        "# The Python interpreter is what calculating those NOT CPU\n",
        "\n",
        "# remember the IEEE-754 ? lol first year trauma\n",
        "# anyway it defines floating point numbers representation in 32bits as follows :\n",
        "# first bit at left is for the sign\n",
        "# the next 8 bits are the exponent\n",
        "# the last are the fraction powers (first bit is 2^-1)\n",
        "\n",
        "# val = (-1)**sign * 2^(E-127) * (1+ SUM[i=1..23, B[23-i]*2^(-i)])\n",
        "# This is more like the scientific representation we studied in highschool or wtver\n",
        "# to represent lets say (+/-)xxx.xx u need to write it in the form :\n",
        "# (-1)^S * x.xxxx * 2^E  ; the bits after the dot are the mantissa bits (23 bs) ,\n",
        "# S is the sign bit\n",
        "\n",
        "\n",
        "# The exponent in the IEEE 754 format is stored as an unsigned integer, but we often need\n",
        "# both positive and negative exponents to represent small and large numbers.\n",
        "# So, instead of using a signed exponent,\n",
        "# we use an unsigned exponent and add a bias (a fixed number) to shift the range.\n",
        "\n",
        "\n",
        "# This means:\n",
        "# Stored exponent e = actual_exponent + 127\n",
        "# So, to get the real exponent:actual¬†exponent=ùëí‚àí127\n",
        "\n",
        "# floating-point numbers scale much more because\n",
        "# they use the exponent to jump to big or small ranges.\n",
        "\n",
        "\n",
        "# Simple CPU ADD and MUL Instructions Don't Work on Floating-Point Numbers Directly.\n",
        "# Instead, they require specialized hardware and instructions.\n",
        "# Modern CPUs have a Floating Point Unit (FPU) ‚Äî a dedicated part\n",
        "# of the processor that knows\n",
        "# CPUs (like x86) and GPUs use dedicated instructions for floating-point math.\n",
        "\n",
        "\n",
        "# GPUs also support 16-bit floating point number with less precision"
      ],
      "metadata": {
        "id": "Ey_AfYrC7NRM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USUALLY in nn , weights and biases are represented using floating-point numbers\n",
        "# Quantization tries to use integer numbers to represent these two matrices while\n",
        "# maintaining the accuracy of the model\n",
        "# The Goal is to Quantize Input , Weight , Bias into the integer space\n",
        "# so we perform all operations using integer arithmetic\n",
        "# then we take the output , Dequantize , and send it to the next layer\n",
        "# we need the next layer not even realize we quantized the previous\n",
        "# so we should not CHANGE the model's output using quantization\n",
        "# we need a mapping between floating points and ints without losing acc , meaning ...\n",
        "\n",
        "# WELL , as u guessed it , we lose some information ; (info theo bb)\n",
        "# we are trying to `compress` a huge CONITNUOUS domain , into a\n",
        "# small DISCRETE domain [-127,127] , (we usually sacrifice the -128 to obtain a\n",
        "# symetric domain/range)\n",
        "# what we a re trying to do is keep the same distribution as the floating point\n",
        "# numbers with respect to their domain (the actuall domain and not the possible domain)\n",
        "# [MIN(float_weights),MAX(float_weights)] ,\n",
        "# and we keep an ANCHOR that tells us how we scaled and how to scale back called the\n",
        "# `zero_point` (idk if this name only holds for asymemtric quantization)\n",
        "# there are two types of quantization\n",
        "# asymetric : the zero of one representation not necessarly the zero of the other\n",
        "# symetric : the zero is same for both when [min == max]\n",
        "\n",
        "# In asymmetric quantization\n",
        "# Xq = clamp(floor(xf/s),0,2^n -1) ; s = (alpha-beta)/(2^n -1)\n",
        "\n",
        "# alpha the biggest float in the tensor , Beta the smallest\n",
        "# we center using the z parameter\n",
        "# z = round(-1 * (Beta/s))\n",
        "# BIGGEST NUMBER MAPPED TO BIGGEST NUMBER\n",
        "# Smallest number mapped to 0\n",
        "# 0 mapped to the center point z\n",
        "\n",
        "# dequantization  : Xf = s(Xq - z)\n",
        "# we notice some information loss\n",
        "\n",
        "# In symmetric quantization\n",
        "# Xq = clamp(round(Xf/s),-(2^(n-1) -1) , (2^(n-1) -1) )\n",
        "# alpha the biggest value in absolute form\n",
        "# s = abs(alpha)/(2^(n-1) -1)\n",
        "\n"
      ],
      "metadata": {
        "id": "5y2hlnchOJZv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# suppress scientific notation\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate randomly\n",
        "X = np.random.uniform(low=-50,high=150 , size=20)\n",
        "\n",
        "\n",
        "# For debugging purposes , lets make sure the important values are at the beggining\n",
        "X[0] = X.max() + 1\n",
        "X[1] = X.min() - 1\n",
        "X[2] = 0\n",
        "\n",
        "\n",
        "# Only keep two decimal places\n",
        "X = np.round(X,2)\n",
        "\n",
        "\n",
        "print(X)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itK2QO7ssNJK",
        "outputId": "dc2b1f8a-6aba-4d9e-f279-166d53e8c86f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150.62 -50.23   0.   -19.75 149.62 -49.23  63.98 -22.26 -48.74 -32.55\n",
            "  43.99  45.24  52.48 143.28 143.56  26.49  46.81 -17.5  -16.16 133.69]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple str8 forward\n",
        "def clamp(params,lower_bound,upper_bound):\n",
        "    params[params < lower_bound] = lower_bound\n",
        "    params[params > upper_bound] = upper_bound\n",
        "    return params\n",
        "\n",
        "# Xq = clamp(floor(xf/s),0,2^n -1) ; s = (alpha-beta)/(2^n -1)\n",
        "# z = round(-1 * (Beta/s))\n",
        "\n",
        "def asymmetric_quantization(X,num_bits):\n",
        "    s = (X.max() - X.min() )/ (2**num_bits -1)\n",
        "    z = np.round(-1*(X.min()/s))\n",
        "    Xq = clamp(np.round(X/s + z),0 , 2**num_bits -1).astype(np.int32)\n",
        "\n",
        "    return Xq, s, z\n",
        "\n",
        "# dequantization  : Xf = s(Xq - z)\n",
        "def asymmetric_dequantization(Xq,scale,z):\n",
        "    Xf = scale * (Xq - z)\n",
        "\n",
        "    return Xf\n",
        "\n",
        "# Xq = clamp(round(Xf/s),-(2^(n-1) -1) , (2^(n-1) -1) )\n",
        "# alpha the biggest value in absolute form\n",
        "# s = abs(alpha)/(2^(n-1) -1)\n",
        "\n",
        "def symmetric_quantization(X,num_bits):\n",
        "    alpha = np.max(np.abs(X))\n",
        "    s = np.abs(alpha) / (2**(num_bits-1)-1)\n",
        "    lower_bound , upper_bound = -(2**(num_bits-1)-1) , (2**(num_bits-1)-1)\n",
        "    Xq = clamp(np.round(X/s),lower_bound,upper_bound)\n",
        "\n",
        "\n",
        "    return Xq , s\n",
        "\n",
        "# Xf = Xq * scale\n",
        "def symmetric_dequantiztion(Xq,s):\n",
        "    return Xq * s\n",
        "\n",
        "\n",
        "\n",
        "def quantization_error(X,Xq):\n",
        "\n",
        "    return np.mean((X-Xq)**2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "asym_q , asym_s,asym_z = asymmetric_quantization(X,8)\n",
        "sym_q , sym_s = symmetric_quantization(X,8)\n",
        "\n",
        "\n",
        "\n",
        "print(f'original : \\n{np.round(X,2)}\\n\\n')\n",
        "print(f'Asymmetric scale : {asym_s} , zero : {asym_z} \\n{asym_q}\\n\\n')\n",
        "print(f'Symmetric scale : {sym_s}\\n{sym_q}')\n",
        "\n",
        "# Notice in Sym , alot of range is unused\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHhwEVrctNwz",
        "outputId": "47d17757-6b25-47cd-c1b6-67ef3e60232a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original : \n",
            "[150.62 -50.23   0.   -19.75 149.62 -49.23  63.98 -22.26 -48.74 -32.55\n",
            "  43.99  45.24  52.48 143.28 143.56  26.49  46.81 -17.5  -16.16 133.69]\n",
            "\n",
            "\n",
            "Asymmetric scale : 0.7876470588235294 , zero : 64.0 \n",
            "[255   0  64  39 254   1 145  36   2  23 120 121 131 246 246  98 123  42\n",
            "  43 234]\n",
            "\n",
            "\n",
            "Symmetric scale : 1.185984251968504\n",
            "[127. -42.   0. -17. 126. -42.  54. -19. -41. -27.  37.  38.  44. 121.\n",
            " 121.  22.  39. -15. -14. 113.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asym_deq_q = asymmetric_dequantization(asym_q,asym_s,asym_z)\n",
        "sym_deq_q = symmetric_dequantiztion(sym_q,sym_s)\n",
        "\n",
        "\n",
        "print(f'original : \\n{np.round(X,2)}\\n\\n')\n",
        "print(f'Asymmetric :{asym_deq_q}\\n\\n')\n",
        "print(f'Symmetric  : {sym_deq_q}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klni3V9W2D8I",
        "outputId": "904749f1-c447-4c95-d566-6e8b1bb114a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original : \n",
            "[150.62 -50.23   0.   -19.75 149.62 -49.23  63.98 -22.26 -48.74 -32.55\n",
            "  43.99  45.24  52.48 143.28 143.56  26.49  46.81 -17.5  -16.16 133.69]\n",
            "\n",
            "\n",
            "Asymmetric :[150.44058824 -50.40941176   0.         -19.69117647 149.65294118\n",
            " -49.62176471  63.79941176 -22.05411765 -48.83411765 -32.29352941\n",
            "  44.10823529  44.89588235  52.77235294 143.35176471 143.35176471\n",
            "  26.78        46.47117647 -17.32823529 -16.54058824 133.9       ]\n",
            "\n",
            "\n",
            "Symmetric  : [150.62       -49.81133858   0.         -20.16173228 149.43401575\n",
            " -49.81133858  64.04314961 -22.53370079 -48.62535433 -32.0215748\n",
            "  43.88141732  45.06740157  52.18330709 143.50409449 143.50409449\n",
            "  26.09165354  46.25338583 -17.78976378 -16.60377953 134.01622047]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(X,asym_deq_q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKFU6AUl2pze",
        "outputId": "722743c4-2857-4ba0-b166-510b2db273de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.05278839100346078)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(X,sym_deq_q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsAgEXf42ywb",
        "outputId": "4af983c0-94fe-4c66-e36d-99867e5e2b2d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.1063697786595571)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In a trained nn , W , B are quantized , the input is quantized on the fly\n",
        "# but how do we dequantize the Y ?\n",
        "# well we run inference on the model on few inputs and observe typical outputs\n",
        "# to calculate scale and zero , this is called `callibration`\n",
        "# then we can dequantize the output of operations on the other quantized values\n",
        "# using the params we just learned\n",
        "\n",
        "# GPUs actually speed up linear layer using Multiply-Accumulate (MAC)\n",
        "# this op is performed in parallel for every row and column using many MAC blocks\n",
        "# GEMM library"
      ],
      "metadata": {
        "id": "VhQ35gr63Dej"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategies to choosing [Alpha , Beta] (the params of the scale)\n",
        "# - MinMax as we did up . This is sensitive to outliers, the error big for all but outlier\n",
        "# - Percentile , we dont rely on the outlier , the error big for the outlier\n",
        "# - MSE : GRID search to find those two that minimize MSE\n",
        "# - Cross-Entropy : create a proba distribution , choose alpha and beta such that\n",
        "# softmax(X) and softmax(Xq) is the same/very close\n",
        "\n",
        "# in CNN ,for each kernel is better to have alpha and beta , to not throw range\n",
        "\n"
      ],
      "metadata": {
        "id": "HO5bf1WN34cD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "params = np.random.uniform(low=-50,high=150,size=10000)\n",
        "\n",
        "# outlier    VVVV\n",
        "params[-1] = 1000\n",
        "\n",
        "params = np.round(params,2)\n",
        "\n",
        "def asymmetric_quantization_percentile(X,num_bits,percentile=99.99):\n",
        "    alpha = np.percentile(X,percentile)\n",
        "    beta = np.percentile(X,100-percentile)\n",
        "    s = (alpha-beta) / (2**num_bits -10)\n",
        "    z = -1*np.round(beta/s)\n",
        "    lower_bound , upper_bound = 0 , 2**num_bits -1\n",
        "    quantized = clamp(np.round(X/s +z),lower_bound,upper_bound).astype(np.int32)\n",
        "    return quantized , s , z\n",
        "\n",
        "asym_q , asym_s,asym_z = asymmetric_quantization(params,8)\n",
        "asymp_q , asymp_s,asymp_z = asymmetric_quantization_percentile(params,8)\n",
        "\n",
        "print(f'original : \\n{np.round(params,2)}\\n\\n')\n",
        "print(f'Asymmetric scale : {asym_s} , zero : {asym_z} \\n{asym_q}\\n\\n')\n",
        "print(f'Asymmetric scale : {asymp_s} , zero : {asymp_z} \\n{asymp_q}\\n\\n')\n",
        "\n",
        "\n",
        "asym_deq_q = asymmetric_dequantization(asym_q,asym_s,asym_z)\n",
        "asymp_deq_q = asymmetric_dequantization(asymp_q,asymp_s,asymp_z)\n",
        "\n",
        "\n",
        "\n",
        "print(f'original : \\n{np.round(X,2)}\\n\\n')\n",
        "print(f'Asymmetric :{asym_deq_q}\\n\\n')\n",
        "print(f'Asymmetric P  : {asymp_deq_q}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFOCDG0f6IEa",
        "outputId": "da79d7c4-6214-4485-b435-0a18ec8e0f00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original : \n",
            "[ -40.45  130.33   69.88 ...  114.5   137.85 1000.  ]\n",
            "\n",
            "\n",
            "Asymmetric scale : 4.117529411764706 , zero : 12.0 \n",
            "[  2  44  29 ...  40  45 255]\n",
            "\n",
            "\n",
            "Asymmetric scale : 0.8130691219487759 , zero : 61.0 \n",
            "[ 11 221 147 ... 202 231 255]\n",
            "\n",
            "\n",
            "original : \n",
            "[150.62 -50.23   0.   -19.75 149.62 -49.23  63.98 -22.26 -48.74 -32.55\n",
            "  43.99  45.24  52.48 143.28 143.56  26.49  46.81 -17.5  -16.16 133.69]\n",
            "\n",
            "\n",
            "Asymmetric :[ -41.17529412  131.76094118   69.998      ...  115.29082353  135.87847059\n",
            " 1000.55964706]\n",
            "\n",
            "\n",
            "Asymmetric P  : [-40.6534561  130.09105951  69.92394449 ... 114.64274619 138.22175073\n",
            " 157.73540966]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(params[:-1],asym_deq_q[:-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OeHTapA77pE",
        "outputId": "71d62bae-302c-464c-a1e2-5f85238d7e1e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.406541621949046)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(params[:],asymp_deq_q[:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khcS8u708lQ-",
        "outputId": "407678fc-6d69-4d0f-d78b-7e9c9cf8738c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(70.99588247300174)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_error(params[:-1],asymp_deq_q[:-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJwxBFc67-N3",
        "outputId": "84b5ba62-cd6c-4c5b-b04f-791bf9914590"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.05492395100969596)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization after training : Post Training Quantization\n",
        "\n",
        "\"\"\"\n",
        "we need the pretrained model , and some `unlabeled` data .\n",
        "we take the model attach observers , this observers during inference on that data\n",
        "they will calculate statistics on the data such as s and z params  calibrate\n",
        "then we quantize the model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "cw8O0v9kTLdh",
        "outputId": "504e7bb2-d5c1-42af-d5e8-2074269784a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwe need the pretrained model , and some `unlabeled` data .\\nwe take the model attach observers , this observers during inference on that data\\nthey will calculate statistics on the data such as s and z params  calibrate\\nthen we quantize the model\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WcAzxiiTzL4",
        "outputId": "07f1c37a-60ec-4cf8-bb74-c2d914343019"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b2320491d30>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))\n",
        "])\n",
        "\n",
        "mnist_training = datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "\n",
        "train_loader = DataLoader(mnist_training,batch_size=8,shuffle=True)\n",
        "\n",
        "\n",
        "mnist_test = datasets.MNIST(root='./data',train=False,download=True,transform=transform)\n",
        "\n",
        "test_loader = DataLoader(mnist_test,batch_size=8,shuffle=True)\n",
        "\n",
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "pPnXeN0dUEBq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,hidden_size1=100,hidden_size2=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(28*28,hidden_size1)\n",
        "        self.linear2 = nn.Linear(hidden_size1,hidden_size2)\n",
        "        self.linear3 = nn.Linear(hidden_size2,10)\n",
        "        self.relu    = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,28*28)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "9iMbo16yU1HF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net().to(device)"
      ],
      "metadata": {
        "id": "nzbYdmqUVazf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model , train_loader,epochs=5):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        loss_sum = 0\n",
        "        data_iterator = tqdm(train_loader,desc=f'Epocj : {epoch+1}/{epochs}')\n",
        "        batch_number = 0\n",
        "        for data in data_iterator :\n",
        "            batch_number +=1\n",
        "            x,y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x.view(-1,28*28))\n",
        "            batch_loss = loss_fn(output,y)\n",
        "            loss_sum += batch_loss\n",
        "            avg_loss = loss_sum/batch_number\n",
        "            data_iterator.set_postfix(loss=avg_loss)\n",
        "\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "def _print_size_of_model(model):\n",
        "    torch.save(model.state_dict(),\"temp_model.p\")\n",
        "    print(f'Size (KB) :{os.path.getsize(\"temp_model.p\")/1_000}')\n",
        "    os.remove('temp_model.p')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cy30U2s7VgvM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(net,train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_GpWcq8XHRF",
        "outputId": "1ee16275-b061-435f-9621-4a905c730431"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epocj : 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:16<00:00, 97.77it/s, loss=tensor(0.2242, grad_fn=<DivBackward0>)]\n",
            "Epocj : 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:15<00:00, 99.80it/s, loss=tensor(0.1166, grad_fn=<DivBackward0>)]\n",
            "Epocj : 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:13<00:00, 101.43it/s, loss=tensor(0.0904, grad_fn=<DivBackward0>)]\n",
            "Epocj : 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:00<00:00, 123.43it/s, loss=tensor(0.0771, grad_fn=<DivBackward0>)]\n",
            "Epocj : 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [00:59<00:00, 126.60it/s, loss=tensor(0.0678, grad_fn=<DivBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader,desc='Testing..'):\n",
        "            x,y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            output = model(x.view(-1,28*28))\n",
        "\n",
        "            for idx , item in enumerate(output):\n",
        "                # since the index of the label is the value of the label\n",
        "                if torch.argmax(item) == y[idx]:\n",
        "                    correct += 1\n",
        "                total+=1\n",
        "\n",
        "    print(f'Accuracy : {round(correct/total,3)}')"
      ],
      "metadata": {
        "id": "feNDCCaHXSIQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weights Before Quantization')\n",
        "print(net.linear1.weight)\n",
        "print(net.linear1.weight.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jUBCs-PZOZ5",
        "outputId": "78e686d1-1666-4f02-8f11-183f41f29194"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights Before Quantization\n",
            "Parameter containing:\n",
            "tensor([[ 0.0078,  0.0272, -0.0214,  ...,  0.0300,  0.0118,  0.0101],\n",
            "        [ 0.0582,  0.0630,  0.0676,  ...,  0.0577,  0.0720,  0.0481],\n",
            "        [ 0.0008,  0.0359, -0.0123,  ...,  0.0006,  0.0222,  0.0290],\n",
            "        ...,\n",
            "        [ 0.0500,  0.0538,  0.0191,  ...,  0.0138,  0.0330, -0.0039],\n",
            "        [ 0.0084,  0.0165,  0.0485,  ...,  0.0288,  0.0289,  0.0284],\n",
            "        [ 0.0063,  0.0010, -0.0132,  ...,  0.0232, -0.0261, -0.0059]],\n",
            "       requires_grad=True)\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Size of the Model before Quant')\n",
        "_print_size_of_model(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6aYQC-7ZYDZ",
        "outputId": "ffbb4e29-d780-453d-8395-60ecb1827b42"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the Model before Quant\n",
            "Size (KB) :360.998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Before Quantization')\n",
        "test(net,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufr0ZUoeZf4B",
        "outputId": "ce8ffdf9-ff73-41b1-df46-2298a282873b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Quantization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing..: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:03<00:00, 359.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we insert min-max observers in the model\n",
        "\n",
        "# QuantStub(): Marks the point where float inputs should be quantized into int8.\n",
        "# DeQuantStub(): Marks the point where int8 outputs should be dequantized back into float32.\n",
        "# They're placeholders for inserting quantization logic during calibration or conversion.\n",
        "\n",
        "\n",
        "class QuantizedNet(nn.Module):\n",
        "    def __init__(self,hidden_size1=100,hidden_size2=100):\n",
        "        super().__init__()\n",
        "        # This is just to define in forward where quant starts\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.linear1 = nn.Linear(28*28,hidden_size1)\n",
        "        self.linear2 = nn.Linear(hidden_size1,hidden_size2)\n",
        "        self.linear3 = nn.Linear(hidden_size2,10)\n",
        "\n",
        "        self.relu    = nn.ReLU()\n",
        "        # This is just to define in forward where quant ends\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,28*28)\n",
        "        x = self.quant(x)  # float ‚Üí int8\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        x = self.dequant(x) # int8 ‚Üí float\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "kExOJnDTZhzY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_net = QuantizedNet().to(device)\n",
        "# we copy the floating point params to this template\n",
        "quantized_net.load_state_dict(net.state_dict())\n",
        "# we stop some training techniques because we want the observers to measure\n",
        "# actual inference behaviour\n",
        "quantized_net.eval()\n",
        "\n",
        "# configuration like which observer to use , symmetric or asymetric\n",
        "# default min-max , weights symmetric , activations asymmetric , fbgemm for cpu backend\n",
        "quantized_net.qconfig = torch.ao.quantization.default_qconfig\n",
        "\n",
        "# this insert the observers module into the model\n",
        "# sensors to watch activations  , weights , biases ... during calibration\n",
        "quantized_net = torch.ao.quantization.prepare(quantized_net)\n",
        "\n",
        "# at this point the model still runs in float32\n",
        "# But every QuantStub, Linear, and ReLU ... is\n",
        "# now wrapped with extra logic to track tensor ranges.\n",
        "quantized_net\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoGvKa29sl6Y",
        "outputId": "533f971b-a3da-4b80-c5d2-3bef183ce608"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedNet(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QuantStub and DeQuantStub are entry/exit markers for quantization in your model‚Äôs\n",
        "#  flow.\n",
        "\n",
        "# Observer modules are behind-the-scenes tools inserted by PyTorch to track tensor ranges\n",
        "# and compute the quantization parameters (scale + zero point).\n",
        "\n",
        "# You write the stubs; PyTorch inserts the observers for you during prepare().\n",
        "\n",
        "\n",
        "# QuantStub\tLike a camera lens ‚Äî it lets you switch from one format (float) to\n",
        "# another (int), but doesn‚Äôt decide how.\n",
        "# Observer\tLike a light meter ‚Äî it looks at the scene (tensor) and tells\n",
        "#  the camera what exposure settings (scale/zero_point) to use."
      ],
      "metadata": {
        "id": "fhX_knoGw6o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we calibrate the model now\n",
        "# we can just run inference with test\n",
        "test(quantized_net,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbj9ai8fxP9i",
        "outputId": "3085e402-21f3-49ab-e016-c0c46a0a82b2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing..: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:04<00:00, 255.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the observers have collected some statistics\n",
        "quantized_net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCsFNakLxZxi",
        "outputId": "9c9eba7a-b8c6-4ee3-f297-7d046357d28a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedNet(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=-91.94515991210938, max_val=54.72724151611328)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=-79.45330810546875, max_val=50.38602828979492)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=-86.16403198242188, max_val=30.331398010253906)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# until now we were using the float32 model and some observers to\n",
        "# collect the necessary statistics to actually get  a quantized model\n",
        "\n",
        "quantized_net = torch.ao.quantization.convert(quantized_net)"
      ],
      "metadata": {
        "id": "WmnCJhAVxgG7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esup8UYtxubT",
        "outputId": "fdbd5a53-b36b-4f8c-c3ec-183464538473"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedNet(\n",
              "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
              "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=1.1549007892608643, zero_point=80, qscheme=torch.per_tensor_affine)\n",
              "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=1.0223569869995117, zero_point=78, qscheme=torch.per_tensor_affine)\n",
              "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.9172868728637695, zero_point=94, qscheme=torch.per_tensor_affine)\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weights Before Quantization')\n",
        "print(torch.int_repr(quantized_net.linear1.weight()))\n",
        "print()\n",
        "print()\n",
        "print(net.linear1.weight)\n",
        "print()\n",
        "print()\n",
        "print(torch.dequantize(quantized_net.linear1.weight()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIGudkKPx40l",
        "outputId": "d24b535e-f761-4582-b455-0c96472e2365"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights Before Quantization\n",
            "tensor([[ 1,  2, -2,  ...,  3,  1,  1],\n",
            "        [ 5,  6,  6,  ...,  5,  6,  4],\n",
            "        [ 0,  3, -1,  ...,  0,  2,  3],\n",
            "        ...,\n",
            "        [ 4,  5,  2,  ...,  1,  3,  0],\n",
            "        [ 1,  1,  4,  ...,  3,  3,  3],\n",
            "        [ 1,  0, -1,  ...,  2, -2, -1]], dtype=torch.int8)\n",
            "\n",
            "\n",
            "Parameter containing:\n",
            "tensor([[ 0.0078,  0.0272, -0.0214,  ...,  0.0300,  0.0118,  0.0101],\n",
            "        [ 0.0582,  0.0630,  0.0676,  ...,  0.0577,  0.0720,  0.0481],\n",
            "        [ 0.0008,  0.0359, -0.0123,  ...,  0.0006,  0.0222,  0.0290],\n",
            "        ...,\n",
            "        [ 0.0500,  0.0538,  0.0191,  ...,  0.0138,  0.0330, -0.0039],\n",
            "        [ 0.0084,  0.0165,  0.0485,  ...,  0.0288,  0.0289,  0.0284],\n",
            "        [ 0.0063,  0.0010, -0.0132,  ...,  0.0232, -0.0261, -0.0059]],\n",
            "       requires_grad=True)\n",
            "\n",
            "\n",
            "tensor([[ 0.0111,  0.0223, -0.0223,  ...,  0.0334,  0.0111,  0.0111],\n",
            "        [ 0.0556,  0.0668,  0.0668,  ...,  0.0556,  0.0668,  0.0445],\n",
            "        [ 0.0000,  0.0334, -0.0111,  ...,  0.0000,  0.0223,  0.0334],\n",
            "        ...,\n",
            "        [ 0.0445,  0.0556,  0.0223,  ...,  0.0111,  0.0334,  0.0000],\n",
            "        [ 0.0111,  0.0111,  0.0445,  ...,  0.0334,  0.0334,  0.0334],\n",
            "        [ 0.0111,  0.0000, -0.0111,  ...,  0.0223, -0.0223, -0.0111]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_print_size_of_model(quantized_net)\n",
        "# original size / 4 and some overhead"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2qjZ9TpyOMW",
        "outputId": "4c955173-c21e-4b65-86c5-12c78214dddf"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (KB) :95.394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(quantized_net,test_loader)\n",
        "# Dont judge on this small simple model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70fi1ADiy2o0",
        "outputId": "df1f56f7-9207-4631-842e-000416dcc08c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing..: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:04<00:00, 304.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNTIL NOW WE WERE USING PTQ,\n",
        "# now we try quantization aware training"
      ],
      "metadata": {
        "id": "v5dhJfptzA2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we inert fake modules in the computtionl graph to simulate the effect of\n",
        "# the quantization\n",
        "# we insert -between each layer some special fake ops quantize and\n",
        "#  dequantize operations- we are not relly quantizing , but we do it on the fly\n",
        "# this will introduce quantization error , and hopefully the backward pass of the\n",
        "# loss will be aware of quantization\n",
        "#"
      ],
      "metadata": {
        "id": "jef_ESPazPsB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_A_quantized_net = QuantizedNet().to(device)"
      ],
      "metadata": {
        "id": "dInlTh3dzL3B"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_A_quantized_net.qconfig = torch.ao.quantization.default_qconfig\n",
        "\n",
        "T_A_quantized_net.train()\n",
        "\n",
        "# the previous prepare works on the premise the net is stable\n",
        "# this works on the premise that the net is undergoing training\n",
        "\n",
        "T_A_quantized_net = torch.ao.quantization.prepare_qat(T_A_quantized_net)\n",
        "T_A_quantized_net\n",
        "# model not trained , we already inserted observers which are not calibrated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0rI20et0BHR",
        "outputId": "c3a92d61-ff4c-444f-8b25-56b14c34ccf9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedNet(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(T_A_quantized_net,train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi2zO7E60vG-",
        "outputId": "365c2c30-7dd8-4581-9cbf-5896c6d1a887"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epocj : 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:12<00:00, 102.94it/s, loss=tensor(0.2193, grad_fn=<DivBackward0>)]\n",
            "Epocj : 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:20<00:00, 93.32it/s, loss=tensor(0.1153, grad_fn=<DivBackward0>)] \n",
            "Epocj : 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:12<00:00, 103.97it/s, loss=tensor(0.0890, grad_fn=<DivBackward0>)]\n",
            "Epocj : 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:12<00:00, 103.07it/s, loss=tensor(0.0789, grad_fn=<DivBackward0>)]\n",
            "Epocj : 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [01:16<00:00, 97.42it/s, loss=tensor(0.0660, grad_fn=<DivBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_A_quantized_net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znA5iRoa07fu",
        "outputId": "76dadcef-6e93-436a-9a4d-ac25b8f54121"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedNet(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-1.502736210823059, max_val=0.7772015929222107)\n",
              "    (activation_post_process): MinMaxObserver(min_val=-86.461669921875, max_val=60.17536926269531)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.8394451141357422, max_val=0.5752117037773132)\n",
              "    (activation_post_process): MinMaxObserver(min_val=-76.77043914794922, max_val=52.59672927856445)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.8849999308586121, max_val=0.27469488978385925)\n",
              "    (activation_post_process): MinMaxObserver(min_val=-105.8464584350586, max_val=38.001827239990234)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in QAT we have weight fake quant , observers that collected during training\n",
        "T_A_quantized_net.eval()\n",
        "\n",
        "# now we can quantize the model with stats collected during training\n",
        "T_A_quantized_net = torch.ao.quantization.convert(T_A_quantized_net)"
      ],
      "metadata": {
        "id": "cnndi4iT08q_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_A_quantized_net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXX67DMi1LOZ",
        "outputId": "38983450-6e09-46ac-f26d-c6ca9d4275f9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedNet(\n",
              "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
              "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=1.1546223163604736, zero_point=75, qscheme=torch.per_tensor_affine)\n",
              "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=1.0186392068862915, zero_point=75, qscheme=torch.per_tensor_affine)\n",
              "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=1.132663607597351, zero_point=93, qscheme=torch.per_tensor_affine)\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weights Before Quantization')\n",
        "print(torch.int_repr(T_A_quantized_net.linear1.weight()))\n",
        "print()\n",
        "print()\n",
        "print(T_A_quantized_net.linear1.weight())\n",
        "print()\n",
        "print()\n",
        "print(torch.dequantize(T_A_quantized_net.linear1.weight()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0A10LhD1Orz",
        "outputId": "b957f2f3-7e5c-4e29-e0e6-81a14c974421"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights Before Quantization\n",
            "tensor([[ 2,  4,  2,  ...,  6,  6,  7],\n",
            "        [-2,  3,  0,  ..., -2,  2, -1],\n",
            "        [ 4, -1,  2,  ..., -1,  4,  0],\n",
            "        ...,\n",
            "        [ 0,  1,  4,  ...,  1,  3,  5],\n",
            "        [-1,  0,  0,  ...,  3,  3,  3],\n",
            "        [ 6,  2,  2,  ...,  3,  4,  5]], dtype=torch.int8)\n",
            "\n",
            "\n",
            "tensor([[ 0.0236,  0.0471,  0.0236,  ...,  0.0707,  0.0707,  0.0825],\n",
            "        [-0.0236,  0.0354,  0.0000,  ..., -0.0236,  0.0236, -0.0118],\n",
            "        [ 0.0471, -0.0118,  0.0236,  ..., -0.0118,  0.0471,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0118,  0.0471,  ...,  0.0118,  0.0354,  0.0589],\n",
            "        [-0.0118,  0.0000,  0.0000,  ...,  0.0354,  0.0354,  0.0354],\n",
            "        [ 0.0707,  0.0236,  0.0236,  ...,  0.0354,  0.0471,  0.0589]],\n",
            "       size=(100, 784), dtype=torch.qint8,\n",
            "       quantization_scheme=torch.per_tensor_affine, scale=0.011786166578531265,\n",
            "       zero_point=0)\n",
            "\n",
            "\n",
            "tensor([[ 0.0236,  0.0471,  0.0236,  ...,  0.0707,  0.0707,  0.0825],\n",
            "        [-0.0236,  0.0354,  0.0000,  ..., -0.0236,  0.0236, -0.0118],\n",
            "        [ 0.0471, -0.0118,  0.0236,  ..., -0.0118,  0.0471,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0118,  0.0471,  ...,  0.0118,  0.0354,  0.0589],\n",
            "        [-0.0118,  0.0000,  0.0000,  ...,  0.0354,  0.0354,  0.0354],\n",
            "        [ 0.0707,  0.0236,  0.0236,  ...,  0.0354,  0.0471,  0.0589]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(T_A_quantized_net,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et8Rt0h_1TgV",
        "outputId": "9a799965-3bc8-4dc7-af28-77694a88b84e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing..: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:05<00:00, 241.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Typically QAT performs better than post training quantization\n",
        "# Operation of quantization is not differentiable so how do we go back ?\n",
        "# we usually approximate the gradient\n",
        "# for all the values between alpha and beta get gradient of 1\n",
        "# else gradient 0\n",
        "\n",
        "\n",
        "# the effect of QAT on loss function optimization\n",
        "# when we train model with not quantization , we try to find the minimize the loss\n",
        "# in quantization aware training we also try the minimize the loss , but we want\n",
        "# the local minimum to be more wide\n",
        "#  in post training quantization , the effects of quant dequant can be too big\n",
        "# maybe the curvature (second derivative is high in that spot)\n",
        "# in QAT we want the change of quant and dequant to be small ,\n",
        "# so we find a minimum that has small curvature (small second deriv (my thought))\n",
        "\n",
        "# basically we try to account for the quant dequant error\n"
      ],
      "metadata": {
        "id": "EyxoDyiL1Z7m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}